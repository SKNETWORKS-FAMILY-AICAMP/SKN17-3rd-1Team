# -*- coding: utf-8 -*-
"""
Chative Jobs â€” Gradio RAG + Streaming (íŒ€ í…œí”Œë¦¿ ìœ ì§€ ë²„ì „)
- UI: ëœë”© â†’ PDF ë¯¸ë¦¬ë³´ê¸° + ìš°ì¸¡ ì±— (ê·¸ëŒ€ë¡œ ìœ ì§€)
- ëª¨ë¸: HF(Base) + LoRA ìŠ¤íŠ¸ë¦¬ë°
- ë‚´ë¶€ RAG: ì—…ë¡œë“œ PDF â†’ ë¬¸ë‹¨ â†’ SBERT(384) â†’ ì„¸ì…˜ FAISS
- ì™¸ë¶€ RAG: ./faiss_index/{faiss.index, store.jsonl} ì¡´ì¬ ì‹œ ì‚¬ìš© (OpenAI ì„ë² ë”© ì¿¼ë¦¬)
í•„ìš”:
  pip install -U gradio transformers peft torch faiss-cpu sentence-transformers PyMuPDF pillow openai
"""

import os, re, json, threading, base64, time
from typing import List, Tuple, Optional, Dict
from pathlib import Path
from PIL import Image
import numpy as np
import faiss
import fitz  # PyMuPDF
import gradio as gr
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer
from peft import PeftModel
from sentence_transformers import SentenceTransformer

# ========= ğŸ” í† í° (í•˜ë“œì½”ë”© or í™˜ê²½ë³€ìˆ˜) =========
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")  
HF_TOKEN       = os.getenv("HF_TOKEN", "") 

# ========= HF ëª¨ë¸ =========
BASE        = "K-intelligence/Midm-2.0-Mini-Instruct"
ADAPTER_DIR = "monkey777/midm2-mini-lora-finetune"

# ========= ê²½ë¡œ =========
BASE_DIR = Path(".")
FAISS_DIR = BASE_DIR / "faiss_index"
CONTENT_INDEX_FP = FAISS_DIR / "faiss.index"
CONTENT_META_FP  = FAISS_DIR / "store.jsonl"

# ========= í•˜ì´í¼íŒŒë¼ë¯¸í„° =========
TOPK_INTERNAL = 6
TOPK_CONTENT  = 6
MAX_INTERNAL_CHARS = 1200
MAX_CONTENT_CHARS  = 1500

SYSTEM_PROMPT = (
    "ë„ˆëŠ” ìµœê³ ì˜ ë§ˆì¼€íŒ… ì „ëµê°€ ê²¸ ì¹´í”¼ë¼ì´í„°ë‹¤. "
    "ì œê³µëœ ë‚´ë¶€/ì™¸ë¶€ ì»¨í…ìŠ¤íŠ¸ë¥¼ **ë³µë¶™í•˜ì§€ ë§ê³  ìš”ì•½/ê°€ê³µ**í•´ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê³  ëª…ë£Œí•˜ê²Œ ë‹µí•˜ë¼. "
    "ì»¨í…ìŠ¤íŠ¸ê°€ ë¶€ì¡±í•˜ë©´ ëª¨ë¸ ì§€ì‹ìœ¼ë¡œ ë³´ê°•í•˜ë˜, ê·¼ê±°ê°€ ì•½í•˜ë©´ 'ì¶”ì •:'ìœ¼ë¡œ êµ¬ë¶„í•˜ë¼. "
    "ê°€ëŠ¥í•˜ë©´ ë‹¤ìŒ êµ¬ì¡°ë¥¼ ë”°ë¥´ë¼:\n"
    "## RAG ìš”ì•½(5~8ê°œ)\n"
    "## ì „ëµ(ì„¸ê·¸ë¨¼íŠ¸ë³„: í˜ì¸/ë‹ˆì¦ˆ â†’ ë©”ì‹œì§€ â†’ ì±„ë„/í˜•ì‹)\n"
    "## ì‹¤í–‰ì•ˆ(3~5ê°œ ì‹¤í—˜: ê°€ì„¤Â·ì§€í‘œÂ·ë¦¬ìŠ¤í¬ + ìƒ˜í”Œ ì¹´í”¼)\n"
    "## ì¸¡ì •(KPIì™€ ìˆ˜ì§‘ ë°©ë²•)"
)

OPENAI_EMBED_MODEL = "text-embedding-3-large"  # ì™¸ë¶€ ì¸ë±ìŠ¤ê°€ í•´ë‹¹ ì°¨ì›ìœ¼ë¡œ ìƒì„±ë˜ì—ˆë‹¤ê³  ê°€ì •(ë³´í†µ 3072)

# ========= ë¡œì»¬ ì„ë² ë”©(SBERT 384) =========
_local_embedder: Optional[SentenceTransformer] = None
def get_embedder():
    global _local_embedder
    if _local_embedder is None:
        _local_embedder = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
    return _local_embedder

def emb_texts_local(texts: List[str]) -> np.ndarray:
    embs = get_embedder().encode(texts, convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=False)
    return embs.astype("float32")

def emb_query_local(q: str) -> np.ndarray:
    v = get_embedder().encode([q], convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=False)
    return v.astype("float32")

# ========= OpenAI ì„ë² ë”©(ì™¸ë¶€ ì¸ë±ìŠ¤) =========
def embed_query_openai(q: str) -> Optional[np.ndarray]:
    try:
        from openai import OpenAI
        client = OpenAI(api_key=OPENAI_API_KEY, timeout=15)
        resp = client.embeddings.create(model=OPENAI_EMBED_MODEL, input=[q])
        v = np.array([resp.data[0].embedding], dtype="float32")
        faiss.normalize_L2(v)
        return v
    except Exception as e:
        print(f"[WARN] OpenAI embed failed: {e}")
        return None

# ========= PDF â†’ í…ìŠ¤íŠ¸/ë¬¸ë‹¨ =========
PARA_SPLIT_RE = re.compile(r"(?:\r?\n\s*){2,}")

def pdf_to_text(pdf_bytes: bytes) -> str:
    pages=[]
    with fitz.open(stream=pdf_bytes, filetype="pdf") as doc:
        for i in range(doc.page_count):
            page = doc.load_page(i)
            words = page.get_text("words")
            if words:
                words.sort(key=lambda w: (w[5], w[6], w[7], w[0]))
                lines_map={}
                for x0,y0,x1,y1,txt,bno,lno,wno in words:
                    if not txt: continue
                    lines_map.setdefault((bno,lno), []).append(txt)
                lines = [" ".join(tokens) for _, tokens in sorted(lines_map.items(), key=lambda k:k[0])]
                pages.append("\n".join(lines))
            else:
                pages.append(page.get_text("text") or "")
    return "\n\n".join(pages)

def _letters_digits(s: str):
    letters = sum(1 for ch in s if ('A'<=ch<='Z') or ('a'<=ch<='z') or ('\uAC00'<=ch<='\uD7A3'))
    digits  = sum(1 for ch in s if ch.isdigit())
    return letters, digits

def text_to_paragraphs(text: str, min_chars=120, drop_digit_heavy=True) -> List[str]:
    text = text.replace("\r\n","\n")
    parts = [re.sub(r"[ \t]*\n[ \t]*", " ", p.strip()) for p in PARA_SPLIT_RE.split(text)]
    out=[]
    for p in parts:
        if len(p) < min_chars: continue
        if drop_digit_heavy:
            L,D = _letters_digits(p)
            if D > L: continue
        out.append(re.sub(r"\s+", " ", p).strip())
    return out

# ========= ì™¸ë¶€ ë©”íƒ€ =========
def load_meta(fp: Path) -> List[Dict]:
    assert fp.exists(), f"Meta not found: {fp}"
    if fp.suffix.lower() == ".jsonl":
        out=[]
        with fp.open("r", encoding="utf-8") as f:
            for ln in f:
                if not ln.strip(): continue
                obj=json.loads(ln)
                out.append({"id": obj.get("id",""), "text": obj.get("document") or obj.get("text","")})
        return out
    data=json.loads(fp.read_text(encoding="utf-8"))
    if isinstance(data, list): return data
    if "records" in data:
        return [{"id": r["id"], "text": r.get("document") or r.get("text","")} for r in data["records"]]
    return data

# ========= ê²€ìƒ‰ ìœ í‹¸ =========
def faiss_search(index, meta, qv: np.ndarray, k: int):
    if index is None or meta is None or qv is None: return []
    D,I = index.search(qv, int(k))
    hits=[]
    for d,i in zip(D[0], I[0]):
        if i < 0 or i >= len(meta): continue
        rec = meta[i]
        hits.append({"id": rec.get("id",""), "text": rec.get("text",""), "score": float(d)})
    return hits

def limit_chars(items: List[Dict], key="text", cap=1000) -> List[str]:
    out=[]; used=0
    for it in items:
        t=(it.get(key) or "").strip()
        if not t: continue
        if used + len(t) > cap: break
        out.append(t); used += len(t)
    return out

# ========= í”„ë¡¬í”„íŠ¸ =========
def sanitize_user_text(s: str) -> str:
    if not s: return s
    s = s.replace("\uFFFD", "").replace("\ufeff","").replace("\uFEFF","")
    return s.strip()

def build_rag_block(internal_ctx: List[str], content_ctx: List[str]) -> str:
    def pack(lines: List[str], cap_each: int, max_lines: int, bullet: str) -> List[str]:
        out=[]
        for t in lines:
            t=(t or "").strip()
            if not t: continue
            if len(t)>cap_each: t=t[:cap_each].rstrip()+"â€¦"
            out.append(f"{bullet} {t}")
            if len(out)>=max_lines: break
        return out
    internal_block = "\n".join(pack(internal_ctx, 300, 6, "-")) if internal_ctx else "(ë‚´ë¶€ ì»¨í…ìŠ¤íŠ¸ ì—†ìŒ)"
    content_block  = "\n".join(pack(content_ctx,  280, 6, "-")) if content_ctx  else "(ì™¸ë¶€ ì»¨í…ìŠ¤íŠ¸ ì—†ìŒ)"
    return f"[ë‚´ë¶€ ë°ì´í„°(ìš°ì„ )]\n{internal_block}\n\n[ì™¸ë¶€ ì—°êµ¬(ë³´ê°•)]\n{content_block}\n\nâ€» ì»¨í…ìŠ¤íŠ¸ê°€ ë¶€ì¡±í•˜ë©´ ëª¨ë¸ ì§€ì‹ìœ¼ë¡œ ë³´ì™„í•˜ë¼."

def build_prompt_from_messages(messages: List[Dict], rag_block: str, tok) -> str:
    # ë§ˆì§€ë§‰ user ë©”ì‹œì§€ì— RAG ë¸”ë¡ì„ ë¶™ì—¬ì¤€ë‹¤
    msgs = [{"role":"system","content": SYSTEM_PROMPT}]
    for m in messages[:-1]:
        if m.get("role") in ("user","assistant") and (m.get("content") or "").strip():
            msgs.append({"role": m["role"], "content": m["content"]})
    last = messages[-1]
    merged_user = f"{sanitize_user_text(last.get('content',''))}\n\n{rag_block}"
    msgs.append({"role":"user","content": merged_user})

    if hasattr(tok, "apply_chat_template"):
        return tok.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)

    compiled=[]
    for m in msgs:
        compiled.append(f"## {m['role'].upper()}\n{m['content']}")
    compiled.append("## ASSISTANT\n")
    return "\n\n".join(compiled)

# ========= ëª¨ë¸ ë¡œë“œ =========
tok = AutoTokenizer.from_pretrained(BASE, trust_remote_code=True, token=(HF_TOKEN or None))
base = AutoModelForCausalLM.from_pretrained(
    BASE, dtype="auto", device_map="auto", trust_remote_code=True, token=(HF_TOKEN or None)
)
model = PeftModel.from_pretrained(base, ADAPTER_DIR, token=(HF_TOKEN or None)).eval()
if tok.pad_token_id is None and tok.eos_token_id is not None:
    tok.pad_token = tok.eos_token

# ========= ì™¸ë¶€(ë§ˆì¼€íŒ…) ì¸ë±ìŠ¤ =========
CONTENT_INDEX = None
CONTENT_META  = None
EXT_DIM = None
CONTENT_READY = False
CONTENT_ERR = None
try:
    if CONTENT_INDEX_FP.exists() and CONTENT_META_FP.exists():
        CONTENT_INDEX = faiss.read_index(str(CONTENT_INDEX_FP))
        CONTENT_META  = load_meta(CONTENT_META_FP)
        EXT_DIM = CONTENT_INDEX.d
        CONTENT_READY = True
        print(f"[RAG] ì™¸ë¶€ ì¸ë±ìŠ¤ ë¡œë“œë¨. dim={EXT_DIM}, docs={len(CONTENT_META)}")
    else:
        CONTENT_ERR = "ì™¸ë¶€ ì¸ë±ìŠ¤/ë©”íƒ€ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."
except Exception as e:
    CONTENT_ERR = f"ì™¸ë¶€ ì¸ë±ìŠ¤ ë¡œë“œ ì˜¤ë¥˜: {e}"
    CONTENT_READY = False
    print(f"[WARN] {CONTENT_ERR}")

# ========= ë‚´ë¶€(ì„¸ì…˜) ì¸ë±ìŠ¤ =========
INTERNAL_INDEX: Optional[faiss.IndexFlatIP] = None
INTERNAL_META: List[Dict] = []

def build_internal_index_from_files(files: List) -> str:
    """ì—…ë¡œë“œëœ PDFë“¤ë¡œ ì„¸ì…˜ ì¸ë±ìŠ¤ ìƒì„±(ì‹œê°„ ì¸¡ì •)"""
    global INTERNAL_INDEX, INTERNAL_META
    t0 = time.time()
    if not files:
        INTERNAL_INDEX, INTERNAL_META = None, []
        return "ğŸ“ ì—…ë¡œë“œëœ PDFê°€ ì—†ìŠµë‹ˆë‹¤."

    all_paras=[]
    try:
        for pf in files:
            raw = pf.read() if hasattr(pf, "read") else open(pf.name, "rb").read()
            txt = pdf_to_text(raw)
            all_paras += text_to_paragraphs(txt, min_chars=120, drop_digit_heavy=True)
        if not all_paras:
            INTERNAL_INDEX, INTERNAL_META = None, []
            return "âš ï¸ ë¬¸ë‹¨ ì¶”ì¶œ ê²°ê³¼ê°€ ë¹„ì—ˆìŠµë‹ˆë‹¤."
        vecs = emb_texts_local(all_paras)  # 384ì°¨ì›
        idx = faiss.IndexFlatIP(vecs.shape[1]); idx.add(vecs)
        meta = [{"id": f"internal::{i:06d}", "text": t} for i,t in enumerate(all_paras)]
        INTERNAL_INDEX, INTERNAL_META = idx, meta
        dt = time.time() - t0
        return f"âœ… ë‚´ë¶€ ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ (ë¬¸ë‹¨ {len(all_paras)}, dim={vecs.shape[1]}) â€” {dt:.2f}s"
    except Exception as e:
        INTERNAL_INDEX, INTERNAL_META = None, []
        return f"âŒ ë‚´ë¶€ ì¸ë±ìŠ¤ ì˜¤ë¥˜: {e}"

# ========= ìƒì„±(ìŠ¤íŠ¸ë¦¬ë°) =========
def stream_answer(messages: List[Dict],
                  k_int: int, k_ext: int,
                  cap_int: int, cap_ext: int,
                  max_new_tokens: int, temperature: float, top_p: float,
                  repetition_penalty: float):
    """messages(OpenAI ìŠ¤íƒ€ì¼)ë¥¼ ë°›ì•„ ìŠ¤íŠ¸ë¦¬ë° ìƒì„±"""
    # 1) ì¿¼ë¦¬ í…ìŠ¤íŠ¸
    user_text = (messages[-1].get("content") or "").strip()
    if not user_text:
        yield ""
        return

    # 2) RAG ê²€ìƒ‰
    internal_ctx=[]; content_ctx=[]
    try:
        # ë‚´ë¶€: ë¡œì»¬(384)
        if INTERNAL_INDEX is not None and INTERNAL_META and int(k_int) > 0:
            qv_local = emb_query_local(user_text)
            internal_hits = faiss_search(INTERNAL_INDEX, INTERNAL_META, qv_local, int(k_int))
            internal_ctx = limit_chars(internal_hits, "text", int(cap_int))

        # ì™¸ë¶€: OpenAI ì¿¼ë¦¬ â†’ ì°¨ì›ê²€ì¦
        if CONTENT_READY and CONTENT_INDEX is not None and CONTENT_META is not None and int(k_ext) > 0:
            qv_ext = embed_query_openai(user_text)
            if qv_ext is not None and qv_ext.shape[1] == CONTENT_INDEX.d:
                content_hits = faiss_search(CONTENT_INDEX, CONTENT_META, qv_ext, int(k_ext))
                content_ctx  = limit_chars(content_hits, "text", int(cap_ext))
    except Exception as e:
        print(f"[WARN] RAG search failed: {e}")

    # 3) í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    rag_block = build_rag_block(internal_ctx, content_ctx)
    prompt = build_prompt_from_messages(messages, rag_block, tok)

    # 4) ìŠ¤íŠ¸ë¦¬ë¨¸
    inputs = tok(prompt, return_tensors="pt", truncation=True, max_length=4096)
    inputs.pop("token_type_ids", None)
    inputs = {k: v.to(model.device) for k, v in inputs.items()}

    streamer = TextIteratorStreamer(tok, skip_prompt=True, skip_special_tokens=True)
    gen_kwargs = dict(
        **inputs,
        max_new_tokens=int(max_new_tokens),
        do_sample=True,
        temperature=float(temperature),
        top_p=float(top_p),
        repetition_penalty=float(repetition_penalty),
        eos_token_id=tok.eos_token_id,
        pad_token_id=tok.pad_token_id,
        streamer=streamer,
    )

    th = threading.Thread(target=model.generate, kwargs=gen_kwargs)
    th.start()
    partial = ""
    for new_text in streamer:
        partial += new_text
        yield partial
    th.join()

# ========= PDF ë Œë”ë§(ì¢Œì¸¡ ë¯¸ë¦¬ë³´ê¸°) =========
def render_pdf_to_images_html(file_path: str) -> str:
    try:
        doc = fitz.open(file_path)
        image_htmls = []
        for i, page in enumerate(doc):
            pix = page.get_pixmap(dpi=220)
            img_bytes = pix.tobytes("png")
            encoded = base64.b64encode(img_bytes).decode('utf-8')
            image_htmls.append(
                f'<img src="data:image/png;base64,{encoded}" alt="Page {i+1}" '
                f'style="max-width: 100%; margin-bottom: 10px; border-radius: 8px;"/>'
            )
        return "".join(image_htmls)
    except Exception as e:
        return f"<p style='color: red;'>PDF ë Œë”ë§ ì˜¤ë¥˜: {e}</p>"

# ========= ì•„ë°”íƒ€ =========
if not os.path.exists("assets"):
    os.makedirs("assets")
avatar_path = "assets/steve_jobs.jpg"
if not os.path.exists(avatar_path):
    img = Image.new('RGB', (100, 100), color = '#222222')
    img.save(avatar_path)

# ========= CSS (íŒ€ í…œí”Œë¦¿ ìœ ì§€) =========
app_css = """
@import url('https://cdn.jsdelivr.net/gh/sun-typeface/SUIT/fonts/variable/woff2/SUIT-Variable.css');
body, #app-container { font-family: 'SUIT Variable', sans-serif; background-color: #000000 !important; color: #FFFFFF !important; }
.gradio-container { background-color: #000000 !important; }
#landing-page { padding: 0 !important; margin: 0 !important; }
.section { min-height: 80vh; display: flex; flex-direction: column; justify-content: center; align-items: center; text-align: center; padding: 2rem; box-sizing: border-box; }
.section h1 { font-size: 3.5rem; font-weight: 800; margin-bottom: 1rem; color: #f5f5f7;}
.section h2 { font-size: 2.5rem; font-weight: 700; margin-bottom: 1.5rem; color: #f5f5f7;}
.section p { font-size: 1rem; max-width: 700px; line-height: 1.6; color: #a1a1a6; }
.upload-section { background-color: #1d1d1f; border-radius: 20px; margin: 2rem auto; max-width: 800px; }
#pdf-uploader > label { display: none !important; }
#pdf-uploader { background: transparent !important; border: 2px dashed #48484a !important; border-radius: 15px; padding: 2rem 1rem; min-height: 150px; }
#pdf-uploader .file-drop { background: transparent !important; border: none !important; color: #a1a1a6 !important;
    display: flex; flex-direction: column; justify-content: center; align-items: center; gap: 1rem; }
#pdf-uploader .file-drop svg { color: #a1a1a6 !important; }
#pdf-uploader .file-drop button { color: #a1a1a6 !important; background: none !important; text-decoration: underline; }
#pdf-uploader .file-list { background: transparent !important; border: none !important; box-shadow: none !important; }
#pdf-uploader .file-list .file { background-color: transparent !important; color: #a1a1a6 !important; border: none !important; border-radius: 8px !important; }
#pdf-uploader .file-list .file .progress-bar { background-color: transparent !important; }
#pdf-uploader .file-list .file .progress-bar .progress { background-color: #007aff !important; height: 4px !important; }
#main-app-interface { background-color: #F0F2F6 !important; color: #000 !important; border-radius: 20px; padding: 1rem;}
#main-app-interface .gradio-container { background-color: transparent !important; }
#pdf-display-container h3 { color: #1d1d1f; }
#chatbot .user { background-color: #007aff !important; border-radius: 18px 18px 5px 18px !important; }
#chatbot .user .prose * { color: #FFFFFF !important; font-weight: bold; }
#chatbot .bot { background-color: #e5e5ea !important; color: #000 !important; border-radius: 18px 18px 18px 5px !important; }
#chatbot .message { border: none !important; box-shadow: none !important; }
#main-app-interface .gr-button { background-color: #007aff; color: white; }
#main-app-interface .gr-button.secondary { background-color: #e5e5ea; color: #000; }
#pdf-display-container { background-color: white; border-radius: 12px; padding: 1rem; overflow-y: auto; height: 93vh; }
#chatbot-container { background-color: #ffffff; border-radius: 12px; padding: 1rem; height: 93vh; display: flex; flex-direction: column; }
#chatbot { flex-grow: 1; min-height: 65vh; }
#chatbot .message p { font-size: 14px !important; line-height: 1.6 !important; }
#chatbot .message .prose * { font-size: 14px !important; line-height: 1.6 !important; }
#chatbot-container textarea { font-size: 14px !important; }
"""

# ========= Gradio UI =========
with gr.Blocks(css=app_css, title="Chative Jobs") as demo:

    # States
    st_messages = gr.State([])           # [{role, content}]
    st_uploaded_pdf_path = gr.State(None)

    # --- Landing ---
    with gr.Column(elem_id="landing-page") as landing_page:
        gr.HTML("""
        <div class="section">
            <h1>Chative Jobs</h1>
            <p style="font-size: 1.5rem; color: #f5f5f7;">ë§ˆì¼€íŒ… ì „ëµ, ë‹¤ì‹œ ìƒê°í•  ì‹œê°„.</p>
        </div>
        <div class="section">
            <h2>ì „ëµì˜ ë³¸ì§ˆ. ì¡ìŠ¤ì˜ ì–¸ì–´.</h2>
            <p>íŒŒì¸íŠœë‹ìœ¼ë¡œ ì™„ì„±ëœ ì¡ìŠ¤ì˜ ë‰´ëŸ´ ì—”ì§„.<br>
            ìˆ˜ë°±ë§Œ ë‹¨ì–´ì—ì„œ ì¶”ì¶œí•œ ê·¸ì˜ ì² í•™. ìˆ˜ì‹­ ë²ˆì˜ í‚¤ë…¸íŠ¸ì—ì„œ ì¦ëª…ëœ ê·¸ì˜ í™”ë²•.<br>
            ìˆ˜ì‹­ í†µì˜ ì´ë©”ì¼ì—ì„œ ë“œëŸ¬ë‚œ ê·¸ì˜ ë³¸ì§ˆ.</p>
        </div>
        <div class="section">
            <h2>ê¶ê·¹ì˜ ë¸Œëœë”© í”¼ë“œë°± ì‹œìŠ¤í…œ.</h2>
            <p>ì–´ë–¤ ê¸°íšì„œë“ , ê·¸ í•µì‹¬ì„ ê¿°ëš«ëŠ” í†µì°°ë ¥.<br>
            ì—¬ê¸°ì— ì—…ê³„ ì‚¬ìƒ ê°€ì¥ ì •ì§í•œ í”¼ë“œë°±ê¹Œì§€.</p>
        </div>
        <div class="section">
            <h2>ë‹¹ì‹ ì˜ ë¹„ì „. ë” ë‹¨ë‹¨í•˜ê²Œ.</h2>
            <p>ìƒˆë¡­ê²Œ ì„ ë³´ì´ëŠ” Chative JobsëŠ” ë‹¹ì‹ ì˜ ì•„ì´ë””ì–´ë¥¼ ìœ„í•œ ë‹´ê¸ˆì§ˆì˜ ê³¼ì •ì…ë‹ˆë‹¤.<br>
            ë³µì¡í•œ ê¸°ëŠ¥ ëª©ë¡ì„ ëŠ˜ì–´ë†“ëŠ” ëŒ€ì‹ , ì‚¬ìš©ìì—ê²Œ ì „í•  ë‹¨ í•˜ë‚˜ì˜ ê°€ì¹˜ë¥¼ ì°¾ì•„ë‚´ì£ .<br>
            ì†ŒìŒì„ ê±·ì–´ë‚´ê³  ë³¸ì§ˆë§Œ ë‚¨ê¸°ëŠ” ê²ƒ. ìœ„ëŒ€í•œ ì „ëµì€ ë°”ë¡œ ê±°ê¸°ì„œ ì‹œì‘ë˜ë‹ˆê¹Œ.</p>
        </div>
        <div class="section">
            <h2>ë‹¨ì§€ AIê°€ ì•„ë‹ˆë‹¤.<br>ì¡ìŠ¤ë‹¤.</h2>
            <p>ë‹¨ìˆœíˆ ì •ë³´ë¥¼ ìš”ì•½í•˜ëŠ” ì±—ë´‡ì€ ë§ìŠµë‹ˆë‹¤.<br>
            í•˜ì§€ë§Œ Chative JobsëŠ” ê´€ì ì„ ì œì‹œí•©ë‹ˆë‹¤.<br>
            ë‹¹ì‹ ì˜ ì•„ì´ë””ì–´ê°€ ì„¸ìƒì„ ë°”ê¿€ ìˆ˜ ìˆëŠ”ì§€, ì•„ë‹ˆë©´ ê·¸ì € ê·¸ëŸ° ì œí’ˆìœ¼ë¡œ ë‚¨ì„ ê²ƒì¸ì§€.<br>
            ê·¸ ì°¨ì´ë¥¼ ë§Œë“­ë‹ˆë‹¤.</p>
        </div>
        <div class="section">
            <h2>ì´ì œ, ë‹¹ì‹ ì˜ ì°¨ë¡€ë‹¤.</h2>
            <p>ë‹¹ì‹ ì˜ ê¸°íšì„œë¥¼ ì—…ë¡œë“œí•˜ê³ ,<br>ê°€ì¥ ìœ„ëŒ€í•œ ë§ˆì¼€í„°ì˜ í”¼ë“œë°±ì„ ê²½í—˜í•˜ì„¸ìš”.</p>
        </div>
        """)
        pdfs = gr.Files(label="ğŸ“ PDF íŒŒì¼ ì—…ë¡œë“œ (ë³µìˆ˜ ì„ íƒ ê°€ëŠ¥)", file_types=[".pdf"], elem_id="pdf-uploader")

    # --- Main App ---
    with gr.Column(visible=False, elem_id="main-app-interface") as main_app:
        with gr.Row():
            # Left: PDF preview
            with gr.Column(scale=5):
                with gr.Column(elem_id="pdf-display-container"):
                    gr.HTML("<h3 style='text-align: center; color: #1d1d1f;'>ğŸ“„ Uploaded Document</h3>")
                    pdf_viewer = gr.HTML()
                    build_log  = gr.Textbox(label="ì¸ë±ìŠ¤ ë¡œê·¸", lines=3)

            # Right: Chat
            with gr.Column(scale=5):
                with gr.Column(elem_id="chatbot-container"):
                    gr.HTML("<h3 style='text-align: center; color: #1d1d1f;'>ğŸ Chative Jobs Feedback</h3>")
                    chatbot = gr.Chatbot(
                        height=600,
                        elem_id="chatbot",
                        type="messages",
                        avatar_images=(None, avatar_path)
                    )
                    user_box = gr.Textbox(placeholder="ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”â€¦", show_label=False)

                with gr.Row():
                    clear_btn = gr.Button("ëŒ€í™” ì§€ìš°ê¸°", variant="secondary")

                with gr.Accordion("RAG ë° ìƒì„± ì˜µì…˜", open=False):
                    with gr.Row():
                        with gr.Column():
                            topk_internal = gr.Slider(0, 20, value=TOPK_INTERNAL, step=1, label="TOPK_INTERNAL (ë‚´ë¶€)")
                            topk_content  = gr.Slider(0, 20, value=TOPK_CONTENT,  step=1, label="TOPK_CONTENT (ì™¸ë¶€)")
                            cap_internal  = gr.Slider(200, 4000, value=MAX_INTERNAL_CHARS, step=100, label="MAX_INTERNAL_CHARS")
                            cap_content   = gr.Slider(200, 4000, value=MAX_CONTENT_CHARS,  step=100, label="MAX_CONTENT_CHARS")
                            gr.Markdown("ì™¸ë¶€ ì¸ë±ìŠ¤ ìƒíƒœ: " + ("âœ… ì‚¬ìš© ê°€ëŠ¥" if CONTENT_READY else f"âŒ ë¯¸ì‚¬ìš© â€” {CONTENT_ERR or 'íŒŒì¼ ì—†ìŒ'}"))
                        with gr.Column():
                            max_new_tokens = gr.Slider(64, 4096, value=1024, step=32, label="max_new_tokens")
                            temperature    = gr.Slider(0.0, 1.5, value=0.7, step=0.05, label="temperature")
                            top_p          = gr.Slider(0.1, 1.0, value=0.9, step=0.05, label="top_p")
                            repetition_penalty = gr.Slider(1.0, 2.0, value=1.05, step=0.01, label="repetition_penalty")

    # ----- Handlers -----

    def on_files_uploaded(files: List, messages: List):
        """ì—…ë¡œë“œ â†’ ë‚´ë¶€ ì¸ë±ìŠ¤ ë¹Œë“œ(ì‹œê°„í‘œì‹œ) + UI ì „í™˜ + ì²« ì•ˆë‚´"""
        log = build_internal_index_from_files(files)

        landing_vis = gr.update(visible=False)
        main_vis    = gr.update(visible=True)

        first_path = files[0].name
        pdf_html = render_pdf_to_images_html(first_path)

        sys_msg = {"role": "assistant", "content": "ë¬¸ì„œ ì—…ë¡œë“œ ì™„ë£Œ. í•µì‹¬ë§Œ ë½‘ì•„ ì „ëµì ìœ¼ë¡œ ë„ìš¸ê²Œìš”. ë¬´ì—‡ì´ ê¶ê¸ˆí•œê°€ìš”?"}
        new_msgs = (messages or []) + [sys_msg]
        return landing_vis, main_vis, pdf_html, log, first_path, new_msgs, new_msgs

    def on_user_submit(user_text: str, messages: List[Dict]):
        """ì‚¬ìš©ì ì…ë ¥ì„ messagesì— ì¶”ê°€ + assistant placeholder ì¶”ê°€(ìŠ¤íŠ¸ë¦¬ë° í‘œì‹œìš©)"""
        if not user_text or not user_text.strip():
            return "", messages, messages
        msgs = (messages or [])
        msgs = msgs + [{"role":"user","content": user_text.strip()}]
        # ìŠ¤íŠ¸ë¦¬ë° ì¤‘ í™”ë©´ ê°±ì‹ ì„ ìœ„í•´ ë¯¸ë¦¬ assistant ë¹ˆ ë©”ì‹œì§€ ì¶”ê°€
        msgs = msgs + [{"role":"assistant","content": ""}]
        return "", msgs, msgs

    def on_bot_stream(messages: List[Dict],
                      k_int, k_ext, cap_int, cap_ext,
                      max_new, temp, tp, rep_pen):
        """ë§ˆì§€ë§‰ assistant ë¹ˆ ë©”ì‹œì§€ë¥¼ ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸"""
        if not messages:
            yield messages
            return
        # ì…ë ¥ìœ¼ë¡œ ì¤„ messages_for_model = ë§ˆì§€ë§‰ assistant ì œê±°(ì¦‰, ë§ˆì§€ë§‰ userê¹Œì§€)
        if messages[-1]["role"] == "assistant":
            messages_for_model = messages[:-1]
        else:
            messages_for_model = messages

        stream = stream_answer(messages_for_model,
                               k_int, k_ext, cap_int, cap_ext,
                               max_new, temp, tp, rep_pen)
        partial = ""
        for chunk in stream:
            partial = chunk
            # ë§ˆì§€ë§‰ì´ assistant placeholderë¼ê³  ê°€ì •í•˜ê³  ì—…ë°ì´íŠ¸
            if messages[-1]["role"] != "assistant":
                messages.append({"role":"assistant","content": ""})
            messages[-1]["content"] = partial
            yield messages

    def on_clear():
        return [], gr.update(visible=True), gr.update(visible=False), None, "", ""

    # ----- Wiring -----

    # ì—…ë¡œë“œ â†’ ì „í™˜ & ì¸ë±ìŠ¤ ë¹Œë“œ & ì´ˆê¸° ì•ˆë‚´
    pdfs.upload(
        on_files_uploaded,
        inputs=[pdfs, st_messages],
        outputs=[landing_page, main_app, pdf_viewer, build_log, st_uploaded_pdf_path, chatbot, st_messages]
    )

    # ì‚¬ìš©ì ì…ë ¥ â†’ messagesì— user+assistant(ë¹ˆ) ì¶”ê°€ â†’ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ assistant ì±„ìš°ê¸°
    user_box.submit(
        on_user_submit,
        inputs=[user_box, st_messages],
        outputs=[user_box, st_messages, chatbot]
    ).then(
        on_bot_stream,
        inputs=[st_messages, topk_internal, topk_content, cap_internal, cap_content,
                max_new_tokens, temperature, top_p, repetition_penalty],
        outputs=[chatbot]
    )

    # ì´ˆê¸°í™”
    clear_btn.click(
        on_clear,
        outputs=[st_messages, landing_page, main_app, st_uploaded_pdf_path, pdf_viewer, build_log]
    )

# ì‹¤í–‰
gr.close_all()
demo.queue().launch(server_name="0.0.0.0", server_port=7860, share=True)
